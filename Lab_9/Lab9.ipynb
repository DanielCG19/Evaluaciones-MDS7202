{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"e97b6303eb884783ab4b125ec2443aee","deepnote_cell_type":"markdown"},"source":["<h1><center>Laboratorio 9: Los huesos de Hip칩crates 游붮</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos</strong></center>"]},{"cell_type":"markdown","metadata":{"cell_id":"0edc4aaa72da4323928ed548b12a4266","deepnote_cell_type":"markdown"},"source":["### Cuerpo Docente:\n","\n","- Profesor: Mat칤as Rojas y Mauricio Araneda\n","- Auxiliar: Ignacio Meza D.\n","- Ayudante: Rodrigo Guerra"]},{"cell_type":"markdown","metadata":{"cell_id":"77a2bff0d1b44e21a49f89d0ee9b19f9","deepnote_cell_type":"markdown"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados\n","\n","- Nombre de alumno 1: Daniel Carmona G.\n","- Nombre de alumno 2: Consuelo Rojas N.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"a734c6f9bc854f0aa636a1fb3e43bee1","deepnote_cell_type":"markdown"},"source":["### **Link de repositorio de GitHub:** `https://github.com/DanielCG19/Evaluaciones-MDS7202`"]},{"cell_type":"markdown","metadata":{"cell_id":"3b249b8e9fd1436a8349b4c5b8a5e4ea","deepnote_cell_type":"markdown"},"source":["### Indice \n","\n","1. [Temas a tratar](#Temas-a-tratar:)\n","3. [Descripcci칩n del laboratorio](#Descripci칩n-del-laboratorio.)\n","4. [Desarrollo](#Desarrollo)"]},{"cell_type":"markdown","metadata":{"cell_id":"9fc800926cdb4acda15d8895c6c330ad","deepnote_cell_type":"markdown"},"source":["## Temas a tratar\n","\n","- Creaci칩n de clasificadores de imagenes a traves de redes Fully connected y CNN.\n","- Uso de Dataloaders para la carga de datasets.\n","- Comparaci칩n de Fully Connected y red convolucional.\n","\n","## Reglas:\n","\n","- Fecha de entrega: 17/11/2022\n","- **Grupos de 2 personas**\n","- **Ausentes** deber치n realizar la actividad solos. \n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n","- Prohibidas las copias. \n","- Pueden usar cualquier material del curso que estimen conveniente.\n","\n","### Objetivos principales del laboratorio\n","\n","- Creaci칩n de modelos de clasificaci칩n de im치genes utilizando Pytorch.\n","- Creaci칩n de dataloader y aplicar transformaciones sobre el dataset.\n","- Comprender la diferencia entre una CNN y una Fully Connected.\n","\n","El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones de `Pytorch`, la cual, est치 enfocada para proyectos de Deep Learning.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"6e3cf55a23354cf190b180a62561314f","deepnote_cell_type":"markdown"},"source":["# Importamos librerias utiles 游땾\n","\n","Comenzamos importando librer칤as utiles para la ejecuci칩n del laboratorio:"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"541f1087b1774136b30d5cd88eb3c723","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3166,"execution_start":1668553671929,"scrolled":true,"source_hash":"6534f170"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n","  warn(f\"Failed to load image Python extension: {e}\")\n"]}],"source":["import os\n","import time\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision import datasets, models\n","from torchvision import transforms as T\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"cell_id":"a2efab4e23cc495cb8e13ce48cc6a8c6","deepnote_cell_type":"markdown"},"source":["# Identificando los Huesos de Hip칩crates游댍\n","\n","<p align=\"center\">\n","  <img src=\"https://i.pinimg.com/originals/d8/58/66/d85866cd1cc3979f94526551addf74b4.gif\" width=\"300\">\n","</p>\n","\n","Tras el 칠xito que han tenido proponiendo modelos de machine learning en trabajos anteriores, el famoso medico Hip칩crates se ha contactado con ustedes para solicitarles ayuda para automatizar la identificaci칩n de radiograf칤as de partes humanas. Para esto, les se침ala que le gustar칤a utilizar algoritmos de deep learning producto que Dem칩crito le se침alo que resultan la mejor alternativa para la predicci칩n de im치genes.\n","\n","En su conversaci칩n con el medico usted le comenta que ha tenido algunas clases relacionadas a Deep Learning, por esto, est치n motivados en abordar el problema utilizando redes Fully Connected y redes convolucionales con Pytorch. Sin embargo, al anunciarle los tipos de redes que conocen, el fil칩sofo les comenta que no hab칤a escuchado muy buenos resultados por parte de las CNN, por lo que les pide que comprueben a traves de la m칠trica de accuracy que tipo de redes es mejor para la tarea de identificaci칩n de radiograf칤as. 쯉er치 cierto lo que dice el fil칩sofo?, Ve치moslo en un nuevo cap칤tulo de los Laboratorios de Programaci칩n Cient칤fica para Ciencia de Datos!"]},{"cell_type":"markdown","metadata":{"cell_id":"80903c45e97e4df8a298c4dac44d6726","deepnote_cell_type":"markdown"},"source":["## 1.1 Creaci칩n de Lista de Archivos\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/BJ-9w-MUVCMAAAAM/tis100-sad.gif\" width=\"300\">\n","</p>\n"]},{"cell_type":"markdown","metadata":{"cell_id":"c771182226214a5c817befd3ab4f16bb","deepnote_cell_type":"markdown"},"source":["Comience revisando de forma manual cada una de las im치genes que posee la carpeta subida a material docente. Verifique la cantidad de tipos de radiograf칤as que se tienen y la cantidad de im치genes que dispone cada carpeta.\n","\n","Revisado el contenido de las im치genes, utilice `os.listdir` para crear un `numpy.array` o un `Dataframe` que contenga las im치genes y un label que se침ale al tipo de radiograf칤a a la que hace referencia la imagen. Para hacer las etiquetas codifique el tipo de im치genes en n칰meros que vayan del 0 al total de tipos de radiograf칤as, no utilice strings para codificar las etiquetas.\n","\n","**Ejemplo de Estructura:**"]},{"cell_type":"markdown","metadata":{"cell_id":"2852df884659483c978efaeac54d2a69","deepnote_cell_type":"markdown"},"source":["<style type=\"text/css\">\n",".tg  {border-collapse:collapse;border-spacing:0;}\n",".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n","  overflow:hidden;padding:10px 5px;word-break:normal;}\n",".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n","  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",".tg .tg-0lax{text-align:left;vertical-align:top}\n","</style>\n","<table class=\"tg\">\n","<thead>\n","  <tr>\n","    <th class=\"tg-0lax\"></th>\n","    <th class=\"tg-0lax\">image_path</th>\n","    <th class=\"tg-0lax\">label</th>\n","  </tr>\n","</thead>\n","<tbody>\n","  <tr>\n","    <td class=\"tg-0lax\">0</td>\n","    <td class=\"tg-0lax\">image1</td>\n","    <td class=\"tg-0lax\">1</td>\n","  </tr>\n","  <tr>\n","    <td class=\"tg-0lax\">1</td>\n","    <td class=\"tg-0lax\">image2</td>\n","    <td class=\"tg-0lax\">0</td>\n","  </tr>\n","  <tr>\n","    <td class=\"tg-0lax\">2</td>\n","    <td class=\"tg-0lax\">image3</td>\n","    <td class=\"tg-0lax\">2</td>\n","  </tr>\n","  <tr>\n","    <td class=\"tg-0lax\">3</td>\n","    <td class=\"tg-0lax\">image4</td>\n","    <td class=\"tg-0lax\">0</td>\n","  </tr>\n","  <tr>\n","    <td class=\"tg-0lax\">4</td>\n","    <td class=\"tg-0lax\">image5</td>\n","    <td class=\"tg-0lax\">4</td>\n","  </tr>\n","</tbody>\n","</table>"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"47b8b9f27f31486c9bd1148a58054441","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":614285,"execution_start":1668553675102,"source_hash":"fa59f751","tags":[]},"outputs":[],"source":["#!unzip Medical-MNIST.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"e93233b967b54bedad666c202c689507","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":808,"execution_start":1668554289390,"source_hash":"cd90cb9d"},"outputs":[],"source":["# C칩digo Aqu칤\n","images_array = []\n","images_labels = []\n","for i, category in enumerate(os.listdir(\"Medical-MNIST/\")):\n","    for image in os.listdir(f\"Medical-MNIST/{category}\"):\n","        img = f\"{category}/{image}\"#Image.open(f\"Medical-MNIST/{category}/{image}\")\n","        images_array.append(img)\n","        images_labels.append(i)"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"103fc119bdb2445dbd725291c9df893b","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":10,"sortBy":[]},"deepnote_to_be_reexecuted":false,"execution_millis":630,"execution_start":1668554290205,"source_hash":"57e9666f","tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AbdomenCT/000000.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AbdomenCT/000001.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AbdomenCT/000002.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AbdomenCT/000003.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AbdomenCT/000004.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>58949</th>\n","      <td>HeadCT/009995.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58950</th>\n","      <td>HeadCT/009996.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58951</th>\n","      <td>HeadCT/009997.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58952</th>\n","      <td>HeadCT/009998.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58953</th>\n","      <td>HeadCT/009999.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>58954 rows 칑 2 columns</p>\n","</div>"],"text/plain":["                  image_path  label\n","0      AbdomenCT/000000.jpeg      0\n","1      AbdomenCT/000001.jpeg      0\n","2      AbdomenCT/000002.jpeg      0\n","3      AbdomenCT/000003.jpeg      0\n","4      AbdomenCT/000004.jpeg      0\n","...                      ...    ...\n","58949     HeadCT/009995.jpeg      5\n","58950     HeadCT/009996.jpeg      5\n","58951     HeadCT/009997.jpeg      5\n","58952     HeadCT/009998.jpeg      5\n","58953     HeadCT/009999.jpeg      5\n","\n","[58954 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.DataFrame({\"image_path\":images_array,\"label\":images_labels})\n","df"]},{"cell_type":"markdown","metadata":{"cell_id":"8758894180324644a719fc875493bb8d","deepnote_cell_type":"markdown"},"source":["## 1.2 Creaci칩n de Dataset\n","\n","Tomando en cuenta la estructura de datos desarrollada en el punto 1.1, construya la clase `MedicalDataset()` que cumpla los siguientes puntos:\n","\n","- [ ] Poseer un `__init__` en el que se almacene `estructura` creada en 1.1, la `raiz` de la carpeta y una funci칩n que permita transformar el dataset (de esto no se preocupe mucho, ya que solamente debe almacenar una funci칩n en el atributo).\n","- [ ] La clase debe ser capaz de entregar la cantidad de elementos a traves de `__len__`.\n","- [ ] Debe poseer el m칠todo `__getitem__` que retorne una tupla con la imagen y su correspondiente etiqueta."]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"07910a9e76b441ee93cf1a51425a4bc4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1668554290329,"source_hash":"b0dbf785"},"outputs":[],"source":["# C칩digo Aqu칤\n","class MedicalDataset(Dataset):\n","    def __init__(self, df, raiz, transform):\n","        self.estructura = df\n","        self.raiz = raiz\n","        self.transform = transform\n","    \n","    def __getitem__(self, img_path):\n","        \n","        # print(f\"{self.raiz}{img_path}\")\n","        # Un poco de ayuda para cargar la imagen\n","        image = Image.open(f\"{self.raiz}{img_path}\").convert('RGB')\n","        label = self.estructura.loc[self.estructura['image_path'] == img_path, 'label'].iloc[0] #self.estructura[self.estructura == str(img_path)][\"label\"]\n","        \n","        # Auida para realizar la transformaci칩n\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, label\n","    \n","    def __len__(self):\n","        return len(self.estructura)"]},{"cell_type":"markdown","metadata":{"cell_id":"aa4ab681a36541ecb9a22dafef7d0f24","deepnote_cell_type":"markdown"},"source":["## 1.3 Prueba del MedicalDataset\n","\n","Con la clase construida en el punto 1.2, verifique su funcionamiento cargando el dataset y realizando las transformaciones que entrega la funci칩n `transform_image`. Compruebe a trav칠s de un ejemplo las transformaciones aplicadas en la imagen, comentando la funci칩n que cumple `MedicalDataset` y si es posible observar todas las transformaciones aplicadas con la funci칩n `transform_image`.\n","\n","- [ ] Probar la clase MedicalDataset y aplicando una transformaci칩n de \"train\"\n","- [ ] Plotear un ejemplo del MedicalDataset.\n","\n","**Funci칩n para transformar las imagenes:**"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"9b3bd76f4075467fbb1731255b2628b6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1668554290335,"source_hash":"464be1dc"},"outputs":[],"source":["def transform_image(stage = None):\n","    size = 32\n","    if stage == \"train\":\n","        Tr_img = T.Compose([T.Resize(size = (size,size)), #T.Compose([T.Resize(size = (256,256)),\n","                T.RandomRotation(degrees = (-20,+20)),\n","                T.ToTensor()])\n","        \n","    elif stage == \"test\" or stage == \"val\":\n","        Tr_img = T.Compose([T.Resize(size = (size,size)), \n","                 T.ToTensor()]) # consumen mucha ram\n","        # Tr_img = T.Compose([T.Resize(size = (224,224)), T.ToTensor()]) \n","\n","    return Tr_img"]},{"cell_type":"markdown","metadata":{"cell_id":"d974cef21a994ecbad1634b224c755d3","deepnote_cell_type":"markdown"},"source":["**C칩digo para obtener un ejemplo:**"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"987fda1b95d341f7bbbd3d61159eef19","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":43,"execution_start":1668554290340,"source_hash":"db382519"},"outputs":[],"source":["# Prueba del dataset /work/Medical-MNIST\n","dataset = MedicalDataset(df, \"Medical-MNIST/\", transform_image(\"train\"))"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"2c73fad5e6f845b38e276222bbd12401","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":495,"execution_start":1668554290383,"source_hash":"98529292","tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AbdomenCT/000000.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AbdomenCT/000001.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AbdomenCT/000002.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AbdomenCT/000003.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AbdomenCT/000004.jpeg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>58949</th>\n","      <td>HeadCT/009995.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58950</th>\n","      <td>HeadCT/009996.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58951</th>\n","      <td>HeadCT/009997.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58952</th>\n","      <td>HeadCT/009998.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>58953</th>\n","      <td>HeadCT/009999.jpeg</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>58954 rows 칑 2 columns</p>\n","</div>"],"text/plain":["                  image_path  label\n","0      AbdomenCT/000000.jpeg      0\n","1      AbdomenCT/000001.jpeg      0\n","2      AbdomenCT/000002.jpeg      0\n","3      AbdomenCT/000003.jpeg      0\n","4      AbdomenCT/000004.jpeg      0\n","...                      ...    ...\n","58949     HeadCT/009995.jpeg      5\n","58950     HeadCT/009996.jpeg      5\n","58951     HeadCT/009997.jpeg      5\n","58952     HeadCT/009998.jpeg      5\n","58953     HeadCT/009999.jpeg      5\n","\n","[58954 rows x 2 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset.estructura"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"514a32fe1fc643d2b39aa16078016754","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1656,"execution_start":1668554290472,"source_hash":"8d2a1c36"},"outputs":[{"name":"stdout","output_type":"stream","text":["<map object at 0x7febf40db880>\n","AbdomenCT/000000.jpeg\n","(64, 64)\n"]}],"source":["data_test = iter(dataset.estructura[\"image_path\"])\n","print(data_test)\n","\n","example_image = next(data_test)\n","print(example_image)\n","\n","img = Image.open(\"Medical-MNIST/CXR/007036.jpeg\").convert('RGB')\n","# Utilice plotly para plotear un ejemplo\n","px.imshow(dataset[example_image][0].permute(1, 2, 0))\n","print(img.size)"]},{"cell_type":"markdown","metadata":{"cell_id":"ec43fd027ef94d8b87c5324f12bca44d","deepnote_cell_type":"markdown"},"source":["> Comente que realiza la clase construida y las transformaciones aplicadas."]},{"cell_type":"markdown","metadata":{"cell_id":"2331d4ec7085453197d1ed4dacaa5979","deepnote_cell_type":"markdown"},"source":["## 1.4 Creaci칩n de Clasificadores\n","\n","<p align=\"center\">\n","  <img src=\"https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif\" width=\"300\">\n","</p>\n","\n","A continuaci칩n, deben construir tres clasificadores con los que deber치n verificar cu치l de las arquitecturas posee un mejor desempe침o para la tarea de clasificaci칩n de im치genes. Para la construcci칩n considere los siguientes puntos:\n","\n","- [ ] Se침ale cual es el objetivo del `forward` en este tipo de redes, sea breve para su explicaci칩n.\n","- [ ] Construir una red Fully Connected para solucionar el problema de clasificaci칩n. Para esta parte se le aconseja que rellene el esqueleto dispuesto m치s abajo y que lleva el nombre de `FCClassifier`, en el deber치 rellenar con la dimensi칩n de las capas ocultas y verificar cual ser치 el tama침o de la entrada.\n","- [ ] Construya una red convolucional **simple** (no m치s de una capa convolucional) para la tarea de clasificaci칩n de im치genes, para esto basen su c칩digo en la clase del d칤a `09-11-2022`.\n","- [ ] Crear una red convolucional m치s compleja. Para esta parte tienen completa libertad en la construcci칩n de su red, lo 칰nico que debe cumplir es que sea convolucional.\n","\n","**Esqueletos Propuestos:**"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"28802fc118af493e8d1638aec02a29aa","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1668554292131,"source_hash":"a4c0885f"},"outputs":[],"source":["# class FCClassifier(nn.Module):\n","#     def __init__(self, in_channels, num_classes):\n","#         super(FCClassifier, self).__init__()\n","\n","#         #self.flatten = nn.Flatten()\n","#         self.lin1 = nn.Linear(in_channels, 64)\n","#         self.lin2 = nn.Linear(64, num_classes)\n","        \n","#     def forward(self, x):\n","#         x = F.relu(self.lin1(x))\n","#         x = self.lin2(x)\n","\n","#         return x\n","\n","class FCClassifier(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(FCClassifier, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(in_channels, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"e1b477a4859a4e5c9a55811b119d2a92","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":65,"execution_start":1668554292146,"source_hash":"94e41307"},"outputs":[],"source":["class CNNClassifier1(nn.Module):\n","    def __init__(self,channels=3):\n","        super(CNNClassifier1, self).__init__()\n","        self.conv1 = nn.Conv2d(channels, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(1176, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 6)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x \n","\n","class CNNClassifier2(nn.Module):\n","    def __init__(self,channels=3):\n","        super(CNNClassifier2, self).__init__()\n","        self.conv1 = nn.Conv2d(channels, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 6)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x \n"]},{"cell_type":"markdown","metadata":{"cell_id":"be6be7a2882746058d7b4e8533adfa5e","deepnote_cell_type":"markdown"},"source":["## 1.5 Separando Datos para el Entrenamiento\n","\n","<p align=\"center\">\n","  <img src=\"https://c.tenor.com/Esn7Jif-ZLQAAAAC/separate-square.gif\" width=\"200\">\n","</p>\n","\n","Utilizando un Holdout a su gusto, separe los datos en un conjunto de entrenamiento y de testing. Aplique las transformaciones correspondientes usando `transform_image` para cada conjunto de datos y utilice `torch.utils.data.DataLoader` para crear un objeto iterable del dataset.\n","\n","- [ ] Definir el Holdout a utilizar.\n","- [ ] Separar los datos en un conjunto de entrenamiento y prueba.\n","- [ ] Aplicar las transformaciones correspondientes en cada uno de los dataset.\n","- [ ] Utilizar `DataLoader` de pytorch sobre los dataset."]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"543933419242487d8c5f0089e67dcbf2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1668554292212,"source_hash":"a197d72c"},"outputs":[],"source":["# Separar Datos de Entrenamiento\n","X_train, X_test, y_train, y_test = train_test_split(dataset.estructura[\"image_path\"], \n","                                                    dataset.estructura[\"label\"], \n","                                                    test_size=0.3, \n","                                                    random_state=42)\n","# X_train, X_val, y_train, y_val = train_test_split(X_train, \n","#                                                   y_train, \n","#                                                   test_size=0.2, \n","#                                                   random_state=42)\n","\n","X_train = X_train.reset_index(drop=True)\n","# X_val = X_val.reset_index(drop=True)\n","X_test = X_test.reset_index(drop=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"42c46d21a969422dbdb864d5aea182ed","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":24,"execution_start":1668554292212,"source_hash":"ff1e5e75","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2    7057\n","4    7022\n","3    6984\n","5    6975\n","0    6923\n","1    6306\n","Name: label, dtype: int64\n","0    3077\n","5    3025\n","3    3016\n","4    2978\n","2    2943\n","1    2648\n","Name: label, dtype: int64\n"]}],"source":["print(y_train.value_counts())\n","# print(y_val.value_counts())\n","print(y_test.value_counts())"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"da70248664da4498bce580862fc15971","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":758,"execution_start":1668554292213,"source_hash":"84e59d4c","tags":[]},"outputs":[],"source":["# def open_img_df(df, dataset, stage):\n","#     img_array = []\n","#     dataset.transform = transform_image(stage)\n","#     for img_path in df:\n","#         img_tensor = dataset[img_path][0]\n","#         img_array.append(img_tensor.numpy())\n","#     return np.asanyarray(img_array)\n","\n","def open_img_df(df, dataset, stage):\n","    img_array = []\n","    label_array = []\n","    dataset.transform = transform_image(stage)\n","    for img_path in df:\n","        img_tensor = dataset[img_path]\n","        img_array.append(img_tensor[0].numpy())\n","        label_array.append(img_tensor[1])\n","    return {\"image\": np.asanyarray(img_array), \"label\": np.asanyarray(label_array)}"]},{"cell_type":"markdown","metadata":{},"source":["Esta parte consume mucha ram, por lo que se generaron y guardaron los dataloaders por parte, para luego cargarlos cuando sean necesarios."]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":"0466c98efce64ee88975ad4a04cd59e1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"3cf8e206"},"outputs":[],"source":["# train_set = open_img_df(X_train, dataset, \"train\")\n","# val_set = open_img_df(X_val, dataset, \"val\")\n","# test_set = open_img_df(X_test, dataset, \"test\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Uso de torch.utils.data.DataLoader\n","# x_train_pt = torch.Tensor(train_set)\n","# y_train_pt = torch.Tensor(y_train.values)\n","# train_pt = {\"x\": x_train_pt, \"y\": y_train_pt}\n","# torch.save(train_pt, \"train_loader.pt\")\n","\n","# x_val_pt = torch.Tensor(val_set)\n","# y_val_pt = torch.Tensor(y_val.values)\n","# val_pt = {\"x\": x_val_pt, \"y\": y_val_pt}\n","# torch.save(val_pt, \"val_loader.pt\")\n","\n","# x_test_pt = torch.Tensor(test_set)\n","# y_test_pt = torch.Tensor(y_test.values)\n","# test_pt = {\"x\": x_test_pt, \"y\": y_test_pt}\n","# torch.save(test_pt, \"test_loader.pt\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Se cargan los dataloaders de train, val y test"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# train_pt = torch.load(\"train_loader.pt\")\n","# val_pt = torch.load(\"val_loader.pt\")\n","# test_pt = torch.load(\"test_loader.pt\")\n","\n","# train_loader = torch.utils.data.DataLoader(train_pt, batch_size=32, shuffle=True)\n","# val_loader = torch.utils.data.DataLoader(val_pt, batch_size=32, shuffle=True)\n","# test_loader = torch.utils.data.DataLoader(test_pt, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(X_train, batch_size=32, shuffle=True)\n","# val_loader = torch.utils.data.DataLoader(X_val, batch_size=32, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(X_test, batch_size=32, shuffle=True)"]},{"cell_type":"markdown","metadata":{"cell_id":"2ba51524d35242dd81866cb0abb2ed36","deepnote_cell_type":"markdown"},"source":["## 1.6 Creaci칩n de Funciones de Entrenamiento y Evaluaci칩n\n","\n","<p align=\"center\">\n","  <img src=\"https://www.researchgate.net/publication/319535615/figure/fig3/AS:536187598065664@1504848493070/A-typical-convolutional-neural-network-CNN-Architecture-for-Medical-Image-Classification.png\" width=\"500\">\n","</p>\n","\n","\n","Ya construido todas las funciones y clases necesarias llego el momento m치s importante... probar la red. Para esta secci칩n, ustedes deber치n ser capaces de definir los hiperpar치metros de la red, definir las funciones de perdida a utilizar, se침alar el optimizador a usar y finalmente crear sus funciones para el entrenamiento y prueba. Para realizar esta parte m치s estructurada, seguir los siguientes puntos de forma secuencial:\n","\n","- [ ] Especifique los Hiperpar치metros de las 3 redes. Para esta parte sea claro de su elecci칩n y se침ale el porqu칠 de sus elecciones (o sea justifique el setting de sus hiperpar치metros).\n","- [ ] Defina los modelos a utilizar, el optimizador que utilizar치 para el modelo y se침ale la funci칩n de perdida que utilizar치.\n","- [ ] Explique de forma breve la funci칩n que cumplen los pasos `Backward` y `Descenso del gradiente` en una red neuronal.\n","- [ ] Cree una funci칩n llamado `train` que entrene a los clasificadores. Para esto, recuerde que estos modelos suelen utilizar un n칰mero de 칠pocas, por lo que deber치 generar un proceso iterativo de entrenamiento. Es importante que su funci칩n imprima las `loss` obtenidas por el modelo en cada 칠poca (si gusta puede almacenar estas losses en una lista para luego graficarlas y comparar).\n","- [ ] Dise침e una funci칩n para evaluar el desempe침o de las redes. Para evaluar las redes utilice solamente la m칠trica accuracy (para esto se le recomienda comparar la predicci칩n con el ground truth)"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"8d286fec5e0a412eb37d25cdf97f649c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"5bd473a9"},"outputs":[],"source":["# Especificar hyperpar치metros de las redes\n","in_channels_fc = 3072#64\n","in_channels_cnn = 64\n","num_classes = 6\n","lr = 1e-3\n","batch_size = 32\n","n_epochs = 10"]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"a8a1daeb87814fbb887ff6e1e7df6478","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"f300cd2c"},"outputs":[],"source":["# Red 1\n","model_fc = FCClassifier(in_channels_fc, num_classes)\n","model_cv1 = CNNClassifier1()\n","model_cv2 = CNNClassifier2()\n","\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer_fc = torch.optim.Adam(model_fc.parameters(), lr=lr)\n","optimizer_cv1 = torch.optim.Adam(model_cv1.parameters(), lr=lr)\n","optimizer_cv2 = torch.optim.Adam(model_cv2.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"fbe52beb62114dd5b4eb91a6c185acba","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"4a13a7aa"},"outputs":[],"source":["# train\n","def train(model, device, train_loader, criterion, optimizer, n_epochs, name):\n","    train_losses = []\n","    for epoch in range(n_epochs):\n","        train_loss = 0.0\n","        val_loss = 0.0\n","        model.train()\n","        for idx, data in enumerate(train_loader,0):\n","            data = open_img_df(data, dataset, \"train\")\n","            target = torch.Tensor(data[\"label\"]).to(device).long()\n","            data = torch.Tensor(data[\"image\"]).to(device)\n","            #print(data.shape, target.shape)\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()*data.size(0)\n","        train_loss = train_loss/len(train_loader)\n","        train_losses.append(train_loss)\n","        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","            epoch+1, \n","            train_loss\n","            ))\n","        if epoch % 2 == 0:\n","            torch.save(model.state_dict(), f'{name}.pt')\n","            \n","    model.load_state_dict(torch.load(f'{name}.pt'))\n","\n","    return model, train_losses\n","    \n","def evaluate(model, device, test_loader):\n","    correct = 0\n","    total = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for idx, data in enumerate(test_loader,0):\n","            data = open_img_df(data, dataset, \"test\")\n","            target = torch.Tensor(data[\"label\"]).to(device).long()\n","            data = torch.Tensor(data[\"image\"]).to(device)\n","            \n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","    accu = 100 * correct / total\n","    print(f'Accuracy of the network on the test images: {accu:.6f}')\n","    return accu\n"]},{"cell_type":"markdown","metadata":{"cell_id":"0d33c7acfd0d4599994fa02a85880ebe","deepnote_cell_type":"markdown"},"source":["## 1.7 Comparaci칩n de Resultados\n","\n","<p align=\"center\">\n","  <img src=\"https://media2.giphy.com/media/icJA0VF7ntoEL18Jez/giphy.gif\"  width=\"200\">\n","</p>\n","\n","Construidas las funciones de entrenamiento y evaluaci칩n, entrene a las redes que construyo y compare los resultados obtenidos con todas las redes se침alando cual posee mejor rendimiento. Comente una diferencia entre las redes Fully Connected y CNN podr칤a generar un mejor desempe침o en una u otra en la tarea de clasificaci칩n de im치genes.\n","\n","- [ ] Entrenar las redes.\n","- [ ] Evaluar las redes.\n","- [ ] Comentar los resultados obtenidos y visualizar si existe una diferencia significativa en el rendimiento debido a la naturaleza de la red."]},{"cell_type":"code","execution_count":22,"metadata":{"cell_id":"54fc738555c3461cba50ba96ccdd82e8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"fe0c2ca4"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# from torchvision.models import resnet18, ResNet18_Weights\n","\n","# weights = ResNet18_Weights.DEFAULT\n","# model_fc = resnet18(weights=weights)"]},{"cell_type":"markdown","metadata":{},"source":["### Model FC"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 \tTraining Loss: 5.069453\n","Epoch: 2 \tTraining Loss: 2.016424\n","Epoch: 3 \tTraining Loss: 1.501928\n","Epoch: 4 \tTraining Loss: 1.204253\n","Epoch: 5 \tTraining Loss: 1.016420\n","Epoch: 6 \tTraining Loss: 1.009947\n","Epoch: 7 \tTraining Loss: 0.684351\n","Epoch: 8 \tTraining Loss: 0.690143\n","Epoch: 9 \tTraining Loss: 0.624260\n","Epoch: 10 \tTraining Loss: 0.536049\n"]},{"ename":"ValueError","evalue":"not enough values to unpack (expected 3, got 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn [25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_fc \u001b[39m=\u001b[39m model_fc\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m criterion \u001b[39m=\u001b[39m criterion\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model_fc, train_losses, val_losses \u001b[39m=\u001b[39m train(model_fc, device, train_loader, criterion, optimizer_fc, n_epochs, \u001b[39m\"\u001b[39m\u001b[39mmodel_fc\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"]}],"source":["model_fc = model_fc.to(device)\n","criterion = criterion.to(device)\n","\n","model_fc, train_losses = train(model_fc, device, train_loader, criterion, optimizer_fc, n_epochs, \"model_fc\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test images: 96.590716\n"]},{"data":{"text/plain":["96.59071634533838"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["model_fc = FCClassifier(in_channels_fc, num_classes)\n","model_fc.load_state_dict(torch.load(\"model_fc.pt\"))\n","evaluate(model_fc.to(device), device, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Model CNN 1"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 \tTraining Loss: 57.265249\n","Epoch: 2 \tTraining Loss: 57.266128\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_cv1 \u001b[39m=\u001b[39m model_cv1\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m criterion \u001b[39m=\u001b[39m criterion\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model_cv1, train_losses \u001b[39m=\u001b[39m train(model_cv1, device, train_loader, criterion, optimizer_cv1, n_epochs, \u001b[39m\"\u001b[39m\u001b[39mmodel_cv1\u001b[39m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn [21], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, n_epochs, name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m#print(data.shape, target.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn [11], line 11\u001b[0m, in \u001b[0;36mCNNClassifier1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m) \u001b[39m# flatten all dimensions except batch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model_cv1 = model_cv1.to(device)\n","criterion = criterion.to(device)\n","\n","model_cv1, train_losses = train(model_cv1, device, train_loader, criterion, optimizer_cv1, n_epochs, \"model_cv1\")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test images: 17.102957\n"]},{"data":{"text/plain":["17.102956974048737"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["model_cv1 = CNNClassifier1()\n","model_cv1.load_state_dict(torch.load(\"model_cv1.pt\"))\n","evaluate(model_cv1.to(device), device, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Model CNN 2"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 \tTraining Loss: 4.290250\n","Epoch: 2 \tTraining Loss: 1.156524\n","Epoch: 3 \tTraining Loss: 0.639427\n","Epoch: 4 \tTraining Loss: 0.555956\n","Epoch: 5 \tTraining Loss: 0.368461\n","Epoch: 6 \tTraining Loss: 0.380274\n","Epoch: 7 \tTraining Loss: 0.328557\n","Epoch: 8 \tTraining Loss: 0.224860\n","Epoch: 9 \tTraining Loss: 0.196245\n","Epoch: 10 \tTraining Loss: 0.210943\n"]}],"source":["model_cv2 = model_cv2.to(device)\n","criterion = criterion.to(device)\n","\n","model_cv2, train_losses = train(model_cv2, device, train_loader, criterion, optimizer_cv2, n_epochs, \"model_cv2\")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test images: 99.920846\n"]},{"data":{"text/plain":["99.92084581896307"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["\n","model_cv2 = CNNClassifier2()\n","model_cv2.load_state_dict(torch.load(\"model_cv2.pt\"))\n","evaluate(model_cv2.to(device), device, test_loader)"]},{"cell_type":"markdown","metadata":{"cell_id":"1f5f87f30575423d888f9bc9d02e5845","deepnote_cell_type":"markdown"},"source":["> Comente los resultados"]},{"cell_type":"markdown","metadata":{"cell_id":"fac55ec316044348924913232da2bba2","deepnote_cell_type":"markdown"},"source":["# Conclusi칩n\n","\n","Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/vKSR-ZakVMIAAAAC/pochitadancing-pochita.gif\">\n","</p>"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=720cbf9f-0ee7-43bc-a6f8-5f02f55f6207' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"e36ff23d91da4023975150ac3194d238","kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
